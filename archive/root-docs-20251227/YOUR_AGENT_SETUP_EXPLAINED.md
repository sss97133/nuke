# Your Agent Setup - Exactly How It Works

**Where**: Self-hosted in your Supabase Edge Functions  
**Why**: GitHub Actions are inefficient, Supabase is production-grade

## ğŸ—ï¸ **Your Actual Agent Architecture**

### **1. Database Layer** (Curation Interface)
```
ğŸ“Š curated_sources table
â”œâ”€â”€ 10 premium auction sites pre-configured
â”œâ”€â”€ Priority rankings (1-10)
â”œâ”€â”€ Expected daily vehicles per site
â””â”€â”€ Your curation controls

ğŸ¤– agent_configs table  
â”œâ”€â”€ autonomous-extraction-agent (every 4 hours)
â”œâ”€â”€ daily-production-run (2 AM daily)
â””â”€â”€ source-health-monitor (hourly)

ğŸ“ agent_execution_logs table
â””â”€â”€ Performance tracking and results
```

### **2. Scheduling Layer** (pg_cron)
```sql
-- Built into your Supabase database
SELECT cron.schedule(
  'premium-extraction',
  '0 */4 * * *',  -- Every 4 hours
  $$SELECT trigger_agent_execution('premium-auction-extractor');$$
);
```

### **3. Execution Layer** (Edge Functions)
```
supabase/functions/autonomous-extraction-agent/index.ts
    â†“
1. Reads curated_sources table (your curation)
2. Calls existing scrape-multi-source function  
3. Processes extraction results
4. Updates performance logs
5. Schedules next run
```

### **4. Tools Layer** (Existing Functions)
```
Uses your proven scrape-multi-source function:
â”œâ”€â”€ Firecrawl extraction
â”œâ”€â”€ OpenAI analysis
â”œâ”€â”€ Organization creation
â””â”€â”€ Import queue processing
```

## ğŸ›ï¸ **How You Control It**

### **Curation Commands**:
```sql
-- Add new premium site
INSERT INTO curated_sources (source_name, source_url, priority) 
VALUES ('New Auction', 'https://example.com', 9);

-- Boost successful site
UPDATE curated_sources SET priority = 10 WHERE source_name = 'Cars & Bids';

-- Pause underperforming site
UPDATE curated_sources SET is_active = false WHERE source_name = 'Slow Site';
```

### **Agent Control**:
```sql
-- Increase extraction frequency
UPDATE agent_configs SET schedule_cron = '0 */2 * * *' WHERE agent_name = 'premium-auction-extractor';

-- Adjust targets
UPDATE agent_configs SET config_json = jsonb_set(config_json, '{target_daily_vehicles}', '50000');
```

## ğŸš€ **Why This Beats GitHub Actions**

| Need | Your Setup | GitHub Actions |
|------|------------|----------------|
| **33k vehicles/day** | âœ… No limits | âŒ 6hr timeout |
| **Continuous operation** | âœ… pg_cron | âŒ Job queuing |
| **Database speed** | âœ… Direct access | âŒ API calls |
| **Cost efficiency** | âœ… $25/month | âŒ $345/month |
| **Reliability** | âœ… Production-grade | âŒ Development tool |

## ğŸ“ **Where Everything Lives**

**Agent Code**: `supabase/functions/autonomous-extraction-agent/`  
**Curation Data**: `curated_sources` table in your database  
**Scheduling**: Built-in `pg_cron` in Supabase  
**Secrets**: Supabase Dashboard â†’ Edge Functions â†’ Secrets  
**Monitoring**: `agent_execution_logs` table  

## âœ… **Current Status**

- **âœ… Agents deployed** in your Supabase Edge Functions
- **âœ… Autonomous scheduling** via pg_cron  
- **âœ… 10 premium sites curated** and ready
- **âœ… Uses existing proven functions** (scrape-multi-source)
- **âœ… No GitHub Actions** inefficiency

**Your agents operate autonomously in production-grade Supabase infrastructure.**

**You curate sources and priorities, agents execute extraction consistently.**
