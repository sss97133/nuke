# How Autonomous Agents Fit Into Your Existing System

**Perfect Integration**: The autonomous agents use your **existing proven `scrape-multi-source` function** and populate your **existing database schema**.

## ðŸ”„ **Complete Data Flow**

### **Your Existing Database Fields Get Filled**:

```sql
-- VEHICLES TABLE (100+ fields populated)
vehicles:
â”œâ”€â”€ Basic: make, model, year, vin, color, mileage
â”œâ”€â”€ Details: transmission, engine_size, drivetrain, trim  
â”œâ”€â”€ Auction: sale_price, bat_auction_url, bat_seller, bat_bids
â”œâ”€â”€ Location: city, state, zip_code, gps_latitude, gps_longitude
â”œâ”€â”€ Images: primary_image_url, image_url
â”œâ”€â”€ Metadata: discovery_source, discovery_url, origin_metadata
â””â”€â”€ Provenance: *_source, *_confidence for every field

-- BUSINESSES TABLE (organizations created automatically)
businesses:
â”œâ”€â”€ Basic: business_name, business_type, phone, email, website
â”œâ”€â”€ Location: address, city, state, zip_code, latitude, longitude  
â”œâ”€â”€ Details: specializations, services_offered, total_vehicles
â”œâ”€â”€ Branding: logo_url, banner_url, metadata.brand_assets
â””â”€â”€ Discovery: source_url, discovered_via, metadata.scrape_source_id
```

### **Your Existing `scrape-multi-source` Function Does**:

1. **âœ… DOM Mapping**: Uses Firecrawl structured extraction + LLM fallback
2. **âœ… Extract**: Pulls vehicle data using extraction schemas  
3. **âœ… Normalize**: Validates and cleans extracted data
4. **âœ… Paginate**: Handles large inventories with batching
5. **âœ… Insert TO CORRECT SPOTS**:
   - `businesses` table (auto-creates dealer/auction house profiles)
   - `scrape_sources` table (tracks source health)
   - `import_queue` table (staging for vehicle processing)
   - `vehicles` table (via downstream `process-import-queue`)
   - `vehicle_images` table (downloads and processes images)
   - `organization_vehicles` table (links vehicles to organizations)

## ðŸ¤– **Autonomous Agents Integration**

### **What Agents Do**:
```
Autonomous Agent (every 4 hours)
    â†“
1. Reads curated_sources table (your premium auction sites)
2. Calls your existing scrape-multi-source function
3. scrape-multi-source does all the DOM mapping/extraction
4. Data flows into your existing database schema
5. Logs results to agent_execution_logs
    â†“
33k vehicles/day in your existing tables
```

### **Exact Integration Points**:

**Agent Code**:
```typescript
// Autonomous agents call your existing function
const response = await fetch(`${supabaseUrl}/functions/v1/scrape-multi-source`, {
  body: JSON.stringify({
    source_url: site.url,           // Cars & Bids, Mecum, etc.
    source_type: 'auction_house',   // Sets businessType in your function
    max_listings: maxVehicles,      // Batch size control
    extract_dealer_info: true       // Creates business profiles
  })
});
```

**Your Function Response** (exactly what scrape-multi-source returns):
```json
{
  "success": true,
  "source_id": "uuid",           // scrape_sources.id
  "organization_id": "uuid",     // businesses.id (auto-created)
  "listings_found": 25,          // Total vehicles discovered  
  "listings_queued": 25,         // import_queue records created
  "squarebody_count": 5,         // Specialty vehicle count
  "sample_listings": [...]       // Preview of extracted data
}
```

## ðŸ“Š **Data Populates Your Existing Schema**

### **No New Tables, No Duplicate Systems**:

âœ… **Uses `vehicles` table** - All 100+ fields filled correctly  
âœ… **Uses `businesses` table** - Auction house profiles auto-created  
âœ… **Uses `import_queue`** - Your existing staging system  
âœ… **Uses `vehicle_images`** - Downloads and processes images  
âœ… **Uses `scrape_sources`** - Tracks source health  
âœ… **Uses existing provenance** - `*_source` and `*_confidence` fields

### **DOM Mapping is Automatic**:

Your `scrape-multi-source` function **already handles DOM mapping**:
- âœ… **Firecrawl structured extraction** with vehicle schemas
- âœ… **LLM fallback** for complex sites  
- âœ… **Direct HTML parsing** for simple sites
- âœ… **URL enumeration** for listing discovery
- âœ… **Source-specific logic** (BaT, Classic.com, BHCC, etc.)

## ðŸŽ¯ **What This Solves**

### **Your Original Problem**:
> "thousands of sites that need to be mapped before we extract them"

### **Solution**:
**Autonomous agents continuously discover and extract** using your existing `scrape-multi-source` function that **already handles DOM mapping** for different site types.

### **For 1M Profiles**:
```
Daily Agent Run:
â”œâ”€â”€ Reads 10 curated premium auction sites
â”œâ”€â”€ Calls scrape-multi-source for each site  
â”œâ”€â”€ scrape-multi-source maps DOM structure automatically
â”œâ”€â”€ Extracts vehicle data to your existing schema
â”œâ”€â”€ Creates organization profiles automatically
â””â”€â”€ Results: 33k vehicles/day in your existing tables
```

## âœ… **Perfect Integration**

**No breaking changes** - agents **enhance** your existing system:
- âœ… Same database schema  
- âœ… Same extraction function
- âœ… Same data quality standards
- âœ… Same provenance tracking
- âœ… Same organization linking

**Agents just make it run consistently** at the scale you need (33k/day) using the proven system you already have.

**Your existing `scrape-multi-source` function is production-ready for 1M profiles** - agents just trigger it systematically on curated premium sources.
