# Automated BAT Scraping via GitHub Actions
# Runs every 6 hours, can also be triggered manually
# Runs on GitHub's servers (free, no local resources used)

name: BAT Scrape

on:
  schedule:
    - cron: '0 */6 * * *'  # Every 6 hours (12 AM, 6 AM, 12 PM, 6 PM UTC)
  workflow_dispatch:  # Allow manual trigger from GitHub UI

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
      - name: Validate Secrets
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        run: |
          if [ -z "${SUPABASE_URL:-}" ]; then
            echo "‚ùå Missing required secret: SUPABASE_URL"
            exit 1
          fi
          if [ -z "${SUPABASE_SERVICE_ROLE_KEY:-}" ]; then
            echo "‚ùå Missing required secret: SUPABASE_SERVICE_ROLE_KEY"
            exit 1
          fi
          echo "‚úÖ All required secrets are set"
      
      - name: Trigger BAT Scrape (go-grinder)
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        run: |
          echo "üîç Starting BaT scrape via go-grinder (proven pattern)..."
          
          # Use go-grinder which is the proven, secure pattern for BaT scraping
          # It handles discovery, self-invocation, and time budgets safely
          RESPONSE=$(curl -sS -w "\n%{http_code}" -X POST \
            "${SUPABASE_URL}/functions/v1/go-grinder" \
            -H "Authorization: Bearer ${SUPABASE_SERVICE_ROLE_KEY}" \
            -H "Content-Type: application/json" \
            --max-time 70 \
            -d '{
              "do_seed": true,
              "chain_depth": 6,
              "seed_every": 1,
              "bat_import_batch": 1,
              "max_listings": 800
            }')
          
          HTTP_CODE=$(echo "$RESPONSE" | tail -n1)
          BODY=$(echo "$RESPONSE" | sed '$d')
          
          echo "HTTP Status: $HTTP_CODE"
          echo "Response: $BODY" | head -c 2000
          
          if [ "$HTTP_CODE" -ge 200 ] && [ "$HTTP_CODE" -lt 300 ]; then
            echo ""
            echo "‚úÖ BaT scrape triggered successfully"
            # Parse and display key metrics if available
            echo "$BODY" | grep -o '"discovered_listing_urls":[0-9]*' || echo "Response parsing skipped"
          else
            echo ""
            echo "‚ùå BaT scrape failed (HTTP $HTTP_CODE)"
            exit 1
          fi
      
      - name: Summary
        if: always()
        run: |
          echo "üìä BaT scrape workflow completed"
          echo "The go-grinder function will continue processing via self-invocation"
          echo "Check Supabase logs or scrape_sources table for full results"

