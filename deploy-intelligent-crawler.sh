#!/bin/bash

# Deploy Intelligent Crawler System
# Complete deployment script for the revolutionary pricing system

set -e

echo "ğŸ•·ï¸ Deploying Intelligent Crawler System..."
echo "=========================================="

# Check if we're in the right directory
if [ ! -f "supabase/config.toml" ]; then
    echo "âŒ Error: Please run this script from the project root directory"
    exit 1
fi

# Check if Supabase CLI is installed
if ! command -v supabase &> /dev/null; then
    echo "âŒ Error: Supabase CLI is not installed"
    echo "Install it with: npm install -g supabase"
    exit 1
fi

echo "ğŸ“‹ Step 1: Deploying database schema..."
supabase db push

echo "ğŸ“‹ Step 2: Running intelligent crawler system migration..."
supabase db reset --linked

echo "ğŸ“‹ Step 3: Applying crawler system schema..."
psql "$DATABASE_URL" -f database/intelligent_crawler_system.sql 2>/dev/null || \
supabase db reset --linked && \
echo "âœ… Database schema applied successfully"

echo "ğŸ“‹ Step 4: Deploying Edge Functions..."

# Deploy intelligent-crawler function
echo "  ğŸ“¦ Deploying intelligent-crawler..."
supabase functions deploy intelligent-crawler --no-verify-jwt

# Deploy crawler-scheduler function  
echo "  ğŸ“¦ Deploying crawler-scheduler..."
supabase functions deploy crawler-scheduler --no-verify-jwt

# Deploy auto-price-discovery function (if exists)
if [ -d "supabase/functions/auto-price-discovery" ]; then
    echo "  ğŸ“¦ Deploying auto-price-discovery..."
    supabase functions deploy auto-price-discovery --no-verify-jwt
fi

# Deploy validate-comparable function (if exists)
if [ -d "supabase/functions/validate-comparable" ]; then
    echo "  ğŸ“¦ Deploying validate-comparable..."
    supabase functions deploy validate-comparable --no-verify-jwt
fi

# Deploy ai-condition-pricing function (if exists)
if [ -d "supabase/functions/ai-condition-pricing" ]; then
    echo "  ğŸ“¦ Deploying ai-condition-pricing..."
    supabase functions deploy ai-condition-pricing --no-verify-jwt
fi

echo "ğŸ“‹ Step 5: Setting up environment variables..."

# Set required environment variables for Edge Functions
supabase secrets set OPENAI_API_KEY="$OPENAI_API_KEY" 2>/dev/null || echo "âš ï¸  OPENAI_API_KEY not set"
supabase secrets set AWS_ACCESS_KEY_ID="$AWS_ACCESS_KEY_ID" 2>/dev/null || echo "âš ï¸  AWS_ACCESS_KEY_ID not set"
supabase secrets set AWS_SECRET_ACCESS_KEY="$AWS_SECRET_ACCESS_KEY" 2>/dev/null || echo "âš ï¸  AWS_SECRET_ACCESS_KEY not set"

echo "ğŸ“‹ Step 6: Testing crawler system..."

# Test the intelligent crawler
echo "  ğŸ§ª Testing intelligent-crawler function..."
curl -X POST "${SUPABASE_URL}/functions/v1/intelligent-crawler" \
  -H "Authorization: Bearer ${SUPABASE_ANON_KEY}" \
  -H "Content-Type: application/json" \
  -d '{
    "search_params": {
      "make": "Ford",
      "model": "Bronco", 
      "year": 1974,
      "year_start": 1966,
      "year_end": 1977
    },
    "crawler_mode": "fast",
    "force_refresh": false
  }' > /dev/null 2>&1 && echo "âœ… Intelligent crawler test passed" || echo "âš ï¸  Intelligent crawler test failed"

# Test the scheduler
echo "  ğŸ§ª Testing crawler-scheduler function..."
curl -X POST "${SUPABASE_URL}/functions/v1/crawler-scheduler" \
  -H "Authorization: Bearer ${SUPABASE_ANON_KEY}" \
  -H "Content-Type: application/json" \
  -d '{"action": "health_check"}' > /dev/null 2>&1 && echo "âœ… Crawler scheduler test passed" || echo "âš ï¸  Crawler scheduler test failed"

echo "ğŸ“‹ Step 7: Initializing crawler queue..."

# Initialize some test crawls
psql "$DATABASE_URL" -c "
SELECT schedule_vehicle_crawl(
  (SELECT id FROM vehicles LIMIT 1),
  'daily',
  5
) WHERE EXISTS (SELECT 1 FROM vehicles LIMIT 1);
" 2>/dev/null || echo "âš ï¸  No vehicles found to schedule crawling"

echo "ğŸ“‹ Step 8: Verifying system health..."

# Check system health
HEALTH_CHECK=$(curl -s -X POST "${SUPABASE_URL}/functions/v1/crawler-scheduler" \
  -H "Authorization: Bearer ${SUPABASE_ANON_KEY}" \
  -H "Content-Type: application/json" \
  -d '{"action": "health_check"}' | jq -r '.result.overall_health' 2>/dev/null || echo "unknown")

echo "  ğŸ¥ System health: $HEALTH_CHECK"

echo ""
echo "ğŸ‰ INTELLIGENT CRAWLER SYSTEM DEPLOYED!"
echo "======================================"
echo ""
echo "ğŸ”§ System Features:"
echo "  âœ… Advanced web crawling with anti-detection"
echo "  âœ… Intelligent data normalization & deduplication"
echo "  âœ… Automated scheduling and queue management"
echo "  âœ… Real-time monitoring and health checks"
echo "  âœ… Algorithmic overlay for data processing"
echo "  âœ… Multi-source price discovery"
echo ""
echo "ğŸŒ Available Endpoints:"
echo "  â€¢ POST /functions/v1/intelligent-crawler"
echo "  â€¢ POST /functions/v1/crawler-scheduler"
echo "  â€¢ RPC: crawl_vehicle_now(vehicle_id)"
echo "  â€¢ RPC: schedule_vehicle_crawl(vehicle_id, schedule_type, priority)"
echo "  â€¢ RPC: get_crawler_health()"
echo "  â€¢ RPC: get_crawler_stats()"
echo ""
echo "ğŸ“Š Monitoring:"
echo "  â€¢ Use IntelligentCrawlerDashboard component"
echo "  â€¢ Check crawler_monitoring table for logs"
echo "  â€¢ Monitor crawler_schedule for queue status"
echo ""
echo "ğŸš€ The system will automatically:"
echo "  â€¢ Crawl new vehicles when added"
echo "  â€¢ Process queue every 15 minutes"
echo "  â€¢ Clean up old data daily"
echo "  â€¢ Rotate user agents and IPs"
echo "  â€¢ Apply rate limiting per domain"
echo ""

if [ "$HEALTH_CHECK" = "excellent" ] || [ "$HEALTH_CHECK" = "good" ]; then
    echo "âœ… System is operational and ready!"
else
    echo "âš ï¸  System deployed but may need configuration"
    echo "   Check the dashboard for detailed health status"
fi

echo ""
echo "ğŸ¯ Next Steps:"
echo "  1. Add the IntelligentCrawlerDashboard to your admin panel"
echo "  2. Configure API keys for external services (optional)"
echo "  3. Monitor crawler performance in the dashboard"
echo "  4. Adjust rate limits and schedules as needed"
echo ""
echo "ğŸ’¡ Pro Tip: The system learns and adapts automatically!"
echo "   It will optimize crawling patterns based on success rates."